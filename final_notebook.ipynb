{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning\n",
    "## Final Notebook of project\n",
    "\n",
    "\n",
    "Students: TESTARD Arthur, VERDON Valentin and VERDIER Nahel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. Imports and variable initialization\n",
    "1. Data analysis\n",
    "    - Size analysis, data type, number of features, split into data categories\n",
    "    - NAN analysis\n",
    "    - Correlatiosn analysis \n",
    "    - #Analyse avec le reshaped\n",
    "\n",
    "2. Research leads\n",
    "    - Signal analysis \n",
    "    - Signal treatment model with filter and correlations with noises -> Prophet\n",
    "    - AutoML (keras)\n",
    "    - #Analyse de la linéarité entre les entrées/sortie\n",
    "    - #Mettre une étape pour réguler les données simulées par rapport aux données observées \n",
    "\n",
    "3. Preprocessing: \n",
    "    - Columns selection\n",
    "    - NANs management\n",
    "    - Columns creation\n",
    "    - Normalizations (StandardScaler, Normalizer, RobustScaler, MinMaxScaler)\n",
    "    - Train/Test split\n",
    "    - décaler les dates de sortie / entrée (si j’ai le temps diff x_t et x_t-1 pour prédire y_t)\n",
    "\n",
    "4. Model XGBoost: \n",
    "    - Different models testing (RandomForest / LinearRegressor)\n",
    "    - Hyperparameters study\n",
    "    - Features importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "# import networkx as nx\n",
    "import scipy\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFolder:\n",
    "    def __init__(self, folder_name: str):\n",
    "        self.folder_name: str\n",
    "        self.X_test_estimated: str = f\"{folder_name}/X_test_estimated.parquet\"\n",
    "        self.X_train_estimated: str = f\"{folder_name}/X_train_estimated.parquet\"\n",
    "        self.X_train_observed: str = f\"{folder_name}/X_train_observed.parquet\"\n",
    "        self.train_targets: str | None = f\"{folder_name}/train_targets.parquet\"\n",
    "\n",
    "A = DataFolder(folder_name='A')\n",
    "B = DataFolder(folder_name='B')\n",
    "C = DataFolder(folder_name='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(diff_path: str = ''):\n",
    "    train_a = pd.read_parquet(diff_path + A.train_targets)\n",
    "    train_b = pd.read_parquet(diff_path + B.train_targets)\n",
    "    train_c = pd.read_parquet(diff_path + C.train_targets)\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet(diff_path + A.X_train_estimated)\n",
    "    X_train_estimated_b = pd.read_parquet(diff_path + B.X_train_estimated)\n",
    "    X_train_estimated_c = pd.read_parquet(diff_path + C.X_train_estimated)\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet(diff_path + A.X_train_observed)\n",
    "    X_train_observed_b = pd.read_parquet(diff_path + B.X_train_observed)\n",
    "    X_train_observed_c = pd.read_parquet(diff_path + C.X_train_observed)\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet(diff_path + A.X_test_estimated)\n",
    "    X_test_estimated_b = pd.read_parquet(diff_path + B.X_test_estimated)\n",
    "    X_test_estimated_c = pd.read_parquet(diff_path + C.X_test_estimated)\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research leads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Signal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data were presenting some periodicities, intuitively, one of our first idea were to analyse the different signals we have, starting by our target, `pv_measurement`. However, as we can see on the following plot, the data is not completly cleared, specially on B and C. We will come back to this point in Preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dates_when_constants(df, date_c = 'time', y = 'pv_measurement', delta = { 'days': 3 }):\n",
    "    df = df.copy()\n",
    "    mask_y_change = df[y] != df[y].shift(1)\n",
    "\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    constant_periods = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if not mask_y_change[index]:\n",
    "            if start_date is None:\n",
    "                start_date = row[date_c]\n",
    "            end_date = row[date_c]\n",
    "        else:\n",
    "            if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "                constant_periods.append((start_date, end_date))\n",
    "            start_date = None\n",
    "            end_date = None\n",
    "\n",
    "    if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "        constant_periods.append((start_date, end_date))\n",
    "    return constant_periods\n",
    "\n",
    "def delete_date_range_from_df(df, dates, date_c = 'time'):\n",
    "    df = df.copy()\n",
    "    c = 0\n",
    "    for start_date, end_date in dates:\n",
    "        mask = (df[date_c] >= start_date) & (df[date_c] < end_date)\n",
    "        df = df[~mask]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "delta = { 'hours': 12 * 4}\n",
    "train_a = delete_date_range_from_df(train_a, filter_dates_when_constants(train_a, delta=delta))\n",
    "train_b = delete_date_range_from_df(train_b, filter_dates_when_constants(train_b, delta=delta))\n",
    "train_c = delete_date_range_from_df(train_c, filter_dates_when_constants(train_c, delta=delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(30, 20), sharex=True)\n",
    "\n",
    "train_a[['time','pv_measurement']].set_index('time').plot(ax=axs[0], title='pv_measurement on location A')\n",
    "train_b[['time','pv_measurement']].set_index('time').plot(ax=axs[1], title='pv_measurement on location B')\n",
    "train_c[['time','pv_measurement']].set_index('time').plot(ax=axs[2], title='pv_measurement on location C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, we then tryied to analys this signal with basic components such as Fourier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_transforms(train):\n",
    "    y = train[\"pv_measurement\"].dropna().values\n",
    "    time_diff = train[\"time\"].diff().mean().total_seconds()\n",
    "    sampling_rate = 1 / time_diff\n",
    "\n",
    "    n = len(y)\n",
    "    freq = np.fft.fftfreq(n, 1 / sampling_rate)\n",
    "    fft_y = np.fft.fft(y)\n",
    "    amp_fft_y = np.abs(fft_y)\n",
    "    phase = np.angle(fft_y)\n",
    "    return freq, fft_y, amp_fft_y, phase, sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = { 'A': train_a.dropna(subset='pv_measurement'), 'B': train_b.dropna(subset='pv_measurement'), 'C': train_c.dropna(subset='pv_measurement') }\n",
    "locations = trains.keys()\n",
    "\n",
    "freqs, fft_ys, amp_fft_ys, phases, sampling_rates = [\n",
    "    { \n",
    "        loc: get_fft_transforms(trains[loc])[k] for loc in locations \n",
    "    } \n",
    "    for k in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], amp_fft_ys[loc][:len(freqs[loc])//2])\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], 20 * np.log10(amp_fft_ys[loc][:len(freqs[loc])//2]))\n",
    "    plt.title(f\"Spectrum in magnitude of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude (dB)\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looked on the most important frequencies in those spectrums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_peak_frequencies(amp_fft_y, freq, threshold, loc):\n",
    "    peaks, _ = scipy.signal.find_peaks(amp_fft_y[:len(amp_fft_y)//2], height=threshold)\n",
    "    peak_frequencies = freq[:len(freq)//2][peaks]\n",
    "\n",
    "    period_size = int(1/peak_frequencies[0])\n",
    "    continuous_component = np.mean(trains[loc][\"pv_measurement\"].dropna().values[:period_size])\n",
    "\n",
    "    print(\"Location:\", loc)\n",
    "    print(f'Most important periods (in days): \\n{1 / peak_frequencies / 3600 / 24}')\n",
    "    print(f'Value of the continous component: {continuous_component}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = { 'A': .5e7, 'B': .5e6, 'C': .25e6 }\n",
    "for loc in locations:\n",
    "    print_peak_frequencies(amp_fft_ys[loc], freqs[loc], thresholds[loc], loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First commentaries:\n",
    "\n",
    "We know from the analysis of the nan values that A got the most clean datas in term of `pv_measurement` values. So our analysis will mostly be based on what we see on A. We can notice 3 most important frequencies: one for the year, one for the day and one for a half-day (12 hours). If we look more on the frequency plot, we can notice a most little one frequency (that our threshold impeach us to read it on the last print). This seems to be a peak for a period of 8 hours, according to the code cell bellow.\n",
    "\n",
    "Because B and C are not much clean, we can suppose that the big differencies we found with A comes from the Nan values, which create some empty cells in these frames, which are compensated by increasing the frequency values. However, we did not pay attention to it much at first be because most of our analysis were based on A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_a_1 = np.min(np.where(freqs['A'] > .00003)) \n",
    "freq_a_2 = np.max(np.where(freqs['A'] < .00004))\n",
    "freq_arg = np.argmax(fft_ys['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2])\n",
    "1 / freqs['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2][freq_arg] / 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm what we sayied on B and C compared to A if we look on the differents sampling rates depending on the situation. Theorically, it should be close to one hour ($=3600$ seconds) because our values are measured every hours. But if we look on `1 / sampling_rates['B']` and `1 / sampling_rates['C']` we see that it's more than it for B and C locations. This comes from Nan values and confirms our point above.\n",
    "\n",
    "We can notice that `1 / sampling_rates['B']` is a bit bigger than an hour. We can explain it by the gap of one week between `X_train_observed_a` and `X_train_estimated_a`, which exists as well in `train_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / sampling_rates['A'] / 3600, 1 / sampling_rates['B'] / 3600, 1 / sampling_rates['C'] / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can try is to recalculate the model by the inverse of Fourier's transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fft_ys = { loc: np.fft.ifft(fft_ys[loc]) for loc in locations }\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(trains[loc]['pv_measurement'], label='real pv_measurement')\n",
    "    plt.plot(i_fft_ys[loc], label='ifft pv_measurement')\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a gap created in C but in facts its due to the index. \n",
    "\n",
    "Now the idea is to keep only the most important frequencies in order to have a model which can be written like this:\n",
    "$$y[n] = \\hat{y}[n] + r[n]$$\n",
    "\n",
    "where $n$ is the index of the output, $y[n]$ is the real value of `pv_measurement` at index $n$ (or time $t$), $\\hat{y}[n]$ is the value at index $n$ of the signal filtered predicted by signal analysis and $r[n]$ is the value at index $n$ of the noise created by mostly, the weather, from our inputs `X_train_estimated`, `X_train_observed`, etc. It would be design by a machine learning model. Actually we did not had the time to test this feature entirely, because of a lack of time our goals priotization. So, it is not entirely designed, but we will detail as far as we came to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = { 'A': '2022-10-21', 'B': '2022-03-15', 'C': '2022-04-01'}\n",
    "# end_dates = { 'A': X_train_observed_a['date_forecast'].max(), 'B': X_train_observed_b['date_forecast'].max(), 'C': X_train_observed_c['date_forecast'].max() }\n",
    "\n",
    "start_dates = { 'A': '2020-10-21', 'B': '2020-03-15', 'C': '2020-04-01' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(fft_values, threshold=60, sample_rate=1):\n",
    "\n",
    "    n = len(fft_values)\n",
    "\n",
    "    frequencies = np.fft.fftfreq(n, 1 / sample_rate)\n",
    "    amplitudes = fft_values * (np.abs(fft_values) > threshold)\n",
    "    phases = np.angle(fft_values)\n",
    "    return {\"frequencies\": frequencies, \"amplitudes\": amplitudes, \"phases\": phases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_signal(model, duration,sample_rate):\n",
    "    frequencies = model[\"frequencies\"]\n",
    "    amplitudes = model[\"amplitudes\"]\n",
    "    phases = model[\"phases\"]\n",
    "\n",
    "    t = np.arange(0, duration, 1)\n",
    "    signal = np.zeros(len(t), dtype=np.complex128)\n",
    "    \n",
    "    for freq, amp, phase in tqdm(zip(frequencies, amplitudes, phases)):\n",
    "        signal += amp * np.exp(2j * np.pi * freq * t / sample_rate + phase)\n",
    "    return signal / len(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholds_to_get_n_freq(signal, nb_freq, threshold, step):\n",
    "    assert step > 0\n",
    "    fft = np.fft.fft(signal)\n",
    "    abs_fft = np.abs(fft[:len(fft)//2])\n",
    "\n",
    "    freqs = [ f for f in abs_fft if f > threshold ]\n",
    "    threshold += step\n",
    "    while len(freqs) > nb_freq:\n",
    "        freqs = [ f for f in abs_fft if f > threshold ]\n",
    "        threshold += step\n",
    "    threshold = threshold if len(freqs) > 0 else threshold - step\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtred_signal(signal, nb_freqs, sample_rate, nb_days_to_predict = 0, threshold = 0, scaler = StandardScaler):\n",
    "    scaler_pred = scaler\n",
    "    scaler = scaler()\n",
    "    Y_normed = scaler.fit_transform(np.array(signal['pv_measurement'].dropna()).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    threshold = get_thresholds_to_get_n_freq(signal=Y_normed, nb_freq=nb_freqs, threshold=0, step=.5)\n",
    "    model = get_model(fft_values=np.fft.fft(Y_normed), threshold=threshold, sample_rate=sample_rate)\n",
    "    pred_from_model_data = np.real(reconstruct_signal(model, duration=len(model[\"frequencies\"]) + nb_days_to_predict, sample_rate=sample_rate)) \n",
    "    scaler_pred = scaler_pred()\n",
    "    pred_normed = scaler_pred.fit_transform(pred_from_model_data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    Y_filtred = scaler.inverse_transform(pred_normed.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    # If we want to filter negative values\n",
    "    Y_filtred[Y_filtred < 0] = 0\n",
    "    return Y_filtred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_freqs = 2\n",
    "nb_days_to_predict = 0\n",
    "\n",
    "trains_on_dates = { loc: trains[loc][(trains[loc][\"time\"] < pd.Timestamp(end_dates[loc])) & (trains[loc][\"time\"] >= pd.Timestamp(start_dates[loc]))] for loc in locations }\n",
    "Y_filtred = { loc: get_filtred_signal(trains_on_dates[loc], nb_freqs, sampling_rates[loc], nb_days_to_predict=nb_days_to_predict) for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(Y_filtred[loc], color='b')\n",
    "    plt.plot(np.array(trains_on_dates[loc]['pv_measurement']), color='orange')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we explored different ways to design $\\hat{y}[n]$. The first one is a raw filter on the whole signal. This method were not much efficient. In our researchs we found `prophet`, a Python (and R) library which gives a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = { 'A': X_train_estimated_a['date_forecast'].max(), 'B': X_train_estimated_b['date_forecast'].max(), 'C': X_train_estimated_c['date_forecast'].max() }\n",
    "trains = { loc: trains[loc][trains[loc]['time'] <= end_dates[loc]] for loc in locations}\n",
    "\n",
    "prophet_scalers = { loc: MinMaxScaler() for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_raw_prophet = { loc: trains[loc].dropna(subset='pv_measurement').reset_index().rename(columns={'time': 'ds', 'pv_measurement': 'y'}) for loc in locations }\n",
    "\n",
    "trains_prophet = trains_raw_prophet\n",
    "for loc in locations:\n",
    "    trains_prophet[loc]['y'] = prophet_scalers[loc].fit_transform(np.array(trains_raw_prophet[loc]['y']).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "models_prophet = { loc: Prophet(changepoint_prior_scale=0.05) for loc in locations }\n",
    "predictions_prophet = {}\n",
    "forecast = {}\n",
    "\n",
    "for loc in locations:\n",
    "    models_prophet[loc].fit(trains_prophet[loc])\n",
    "    predictions_prophet[loc] = models_prophet[loc].make_future_dataframe(periods=66, freq='h')\n",
    "    forecast[loc] = models_prophet[loc].predict(predictions_prophet[loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(trains_prophet[loc]['y'], color='orange')\n",
    "    plt.plot(forecast[loc]['yhat'], color='b')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results, from prophet and the filter signal, are not really satisfying. Then came the idea, inpired by [this paper](https://peerj.com/preprints/3190.pdf), to see what's happen if we plot one signal for each hour (it would make $24 * 3 = 72$ models). We then first split our signals by hours and plot what we get with prophet prediction and our filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFULL VALUES TO COMPILE\n",
    "\n",
    "hours = [ f\"0{h}\" if h < 10 else str(h) for h in range(24) ]\n",
    "\n",
    "# end_dates = { 'A': '2022-10-21', 'B': '2022-03-15', 'C': '2022-04-01'}\n",
    "# start_dates = { 'A': '2020-10-21', 'B': '2020-03-15', 'C': '2020-04-01' }\n",
    "nb_freqs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_on_dates = { loc: trains[loc][(trains[loc][\"time\"] < pd.Timestamp(end_dates[loc])) & (trains[loc][\"time\"] >= pd.Timestamp(start_dates[loc]))] for loc in locations }\n",
    "trains_on_hours = { loc: { h: trains_on_dates[loc][trains_on_dates[loc]['time'].dt.strftime('%H:%M:%S').str.endswith(f'{h}:00:00')] for h in hours } for loc in locations }\n",
    "\n",
    "# MAYBE DROP NA ON B AND C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_h_filtred = { loc: { h: get_filtred_signal(trains_on_hours[loc][h], nb_freqs, sampling_rates[loc] * 24, nb_days_to_predict=nb_days_to_predict, scaler=StandardScaler) for h in hours} for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(trains_on_hours[loc][h]['pv_measurement']), color='orange')\n",
    "        plt.plot(Y_h_filtred[loc][h], color='b')\n",
    "        plt.title(f\"Filtred signal at time h={h} for location {loc}\")\n",
    "        sp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hours_to_days(signal):\n",
    "    min_len = np.min([len(signal[h]) for h in hours ])\n",
    "    y_pred = []\n",
    "    for d in range(min_len):\n",
    "        for h in hours:\n",
    "            y_pred.append(signal[h][d])\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_h_filtred[loc]['21'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_train = { loc: convert_hours_to_days(Y_h_filtred[loc]) for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(reconstructed_train[loc], color='b', label='reconstruted from signal analysis')\n",
    "    plt.plot(np.array(trains_on_dates[loc]['pv_measurement']), color='orange', label='real value')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    plt.legend()\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get here a far more satisfying result. There is a problem for B, we did not get why the curve does not go to 0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = { loc: {} for loc in locations }\n",
    "trains_prophet_on_hours = { loc: { h: trains_prophet[loc][trains_prophet[loc]['ds'].dt.strftime('%H:%M:%S').str.endswith(f'{h}:00:00')] for h in hours } for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        date_index = [ (pd.Timestamp(start_dates[loc]) + d).strftime(\"%Y-%m-%d\") for d in [ timedelta(days=k) for k in range(len(trains_prophet_on_hours[loc][h])) ] ]\n",
    "        trains_prophet_on_hours[loc][h] = pd.DataFrame({'ds': date_index, 'y': trains_prophet_on_hours[loc][h]['y']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_prophet = { loc: { h: Prophet(changepoint_prior_scale=0.05) for h in hours} for loc in locations }\n",
    "predictions_prophet = { loc: {} for loc in locations }\n",
    "forecast = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        models_prophet[loc][h].fit(trains_prophet_on_hours[loc][h])\n",
    "        predictions_prophet[loc][h] = models_prophet[loc][h].make_future_dataframe(periods=0, freq='h')\n",
    "        forecast[loc][h] = models_prophet[loc][h].predict(predictions_prophet[loc][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(trains_prophet_on_hours[loc][h].reset_index()['y']), color='orange')\n",
    "        plt.plot(np.array(forecast[loc][h]['yhat']), color='b')\n",
    "        plt.title(f\"Prophet on location {loc}\")\n",
    "        sp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_to_reconstruct = { loc: { h: prophet_scalers[loc].inverse_transform(np.array(forecast[loc][h]['yhat']).reshape(-1, 1)).reshape(-1) for h in hours } for loc in locations }\n",
    "Y_to_reconstruct = { loc: { h: np.array(forecast[loc][h]['yhat']) for h in hours } for loc in locations }\n",
    "reconstructed_train_prophet = { loc: convert_hours_to_days(Y_to_reconstruct[loc]) for loc in locations }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(reconstructed_train_prophet[loc], color='b', label='reconstruted from signal analysis')\n",
    "    plt.plot(np.array(trains_prophet[loc]['y']), color='orange', label='real value')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    plt.legend()\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are less satisfying than the precedent result. But let's see what happens if we try to predict the noise, $r[n]$, with Prophet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prophet = { loc: { h: np.array(trains_on_hours[loc][h]['pv_measurement']) - Y_h_filtred[loc][h] for h in hours } for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prophet = { loc: { h: np.array(trains_on_hours[loc][h]['pv_measurement']) - Y_h_filtred[loc][h] for h in hours } for loc in locations }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_noise_prophet_on_hours = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        date_index = [ (pd.Timestamp(start_dates[loc]) + d).strftime(\"%Y-%m-%d\") for d in [ timedelta(days=k) for k in range(len(noise_prophet[loc][h])) ] ]\n",
    "        trains_noise_prophet_on_hours[loc][h] = pd.DataFrame({'ds': date_index, 'y': noise_prophet[loc][h]})\n",
    "\n",
    "models_prophet = { loc: { h: Prophet(changepoint_prior_scale=0.05) for h in hours} for loc in locations }\n",
    "predictions_prophet = { loc: {} for loc in locations }\n",
    "forecast = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        models_prophet[loc][h].fit(trains_noise_prophet_on_hours[loc][h])\n",
    "        predictions_prophet[loc][h] = models_prophet[loc][h].make_future_dataframe(periods=0, freq='h')\n",
    "        forecast[loc][h] = models_prophet[loc][h].predict(predictions_prophet[loc][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(noise_prophet[loc][h]), color='orange')\n",
    "        plt.plot(np.array(forecast[loc][h]['yhat']), color='b')\n",
    "        plt.title(f\"Prophet on location {loc}\")\n",
    "        sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our study, we explored many ways to pre-process our inputs. Some were compatible to each other, some were not. We are going to present in this part, all we did as pre-process and those we used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Interpolation of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our idea, which worked pretty well, was to interpolate the values of the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Input reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsolar",
   "language": "python",
   "name": "mlsolar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
