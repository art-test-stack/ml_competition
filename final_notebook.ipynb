{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning\n",
    "## Final Notebook of project\n",
    "\n",
    "\n",
    "Students: TESTARD Arthur, VERDON Valentin and VERDIER Nahel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "0. Imports and variable initialization\n",
    "1. Data analysis\n",
    "    - Size analysis, data type, number of features, split into data categories\n",
    "    - NAN analysis\n",
    "    - Correlatiosn analysis \n",
    "    - #Analyse avec le reshaped\n",
    "\n",
    "2. Research leads\n",
    "    - Signal analysis \n",
    "    - Signal treatment model with filter and correlations with noises -> Prophet\n",
    "    - AutoML (keras)\n",
    "    - #Analyse de la linéarité entre les entrées/sortie\n",
    "    - #Mettre une étape pour réguler les données simulées par rapport aux données observées \n",
    "\n",
    "3. Preprocessing: \n",
    "    - Columns selection\n",
    "    - NANs management\n",
    "    - Columns creation\n",
    "    - Normalizations (StandardScaler, Normalizer, RobustScaler, MinMaxScaler)\n",
    "    - Train/Test split\n",
    "    - décaler les dates de sortie / entrée (si j’ai le temps diff x_t et x_t-1 pour prédire y_t)\n",
    "\n",
    "4. Model XGBoost: \n",
    "    - Different models testing (RandomForest / LinearRegressor)\n",
    "    - Hyperparameters study\n",
    "    - Features importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFolder:\n",
    "    def __init__(self, folder_name: str):\n",
    "        self.folder_name: str\n",
    "        self.X_test_estimated: str = f\"{folder_name}/X_test_estimated.parquet\"\n",
    "        self.X_train_estimated: str = f\"{folder_name}/X_train_estimated.parquet\"\n",
    "        self.X_train_observed: str = f\"{folder_name}/X_train_observed.parquet\"\n",
    "        self.train_targets: str | None = f\"{folder_name}/train_targets.parquet\"\n",
    "\n",
    "A = DataFolder(folder_name='A')\n",
    "B = DataFolder(folder_name='B')\n",
    "C = DataFolder(folder_name='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(diff_path: str = ''):\n",
    "    train_a = pd.read_parquet(diff_path + A.train_targets)\n",
    "    train_b = pd.read_parquet(diff_path + B.train_targets)\n",
    "    train_c = pd.read_parquet(diff_path + C.train_targets)\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet(diff_path + A.X_train_estimated)\n",
    "    X_train_estimated_b = pd.read_parquet(diff_path + B.X_train_estimated)\n",
    "    X_train_estimated_c = pd.read_parquet(diff_path + C.X_train_estimated)\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet(diff_path + A.X_train_observed)\n",
    "    X_train_observed_b = pd.read_parquet(diff_path + B.X_train_observed)\n",
    "    X_train_observed_c = pd.read_parquet(diff_path + C.X_train_observed)\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet(diff_path + A.X_test_estimated)\n",
    "    X_test_estimated_b = pd.read_parquet(diff_path + B.X_test_estimated)\n",
    "    X_test_estimated_c = pd.read_parquet(diff_path + C.X_test_estimated)\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research leads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Signal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data were presenting some periodicities, intuitively, one of our first idea were to analyse the different signals we have, starting by our target, `pv_measurement`. However, as we can see on the following plot, the data is not completly cleared, specially on B and C. We will come back to this point in Preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(30, 20), sharex=True)\n",
    "\n",
    "train_a[['time','pv_measurement']].set_index('time').plot(ax=axs[0], title='pv_measurement on location A')\n",
    "train_b[['time','pv_measurement']].set_index('time').plot(ax=axs[1], title='pv_measurement on location B')\n",
    "train_c[['time','pv_measurement']].set_index('time').plot(ax=axs[2], title='pv_measurement on location C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, we then tryied to analys this signal with basic components such as Fourier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_transforms(train):\n",
    "    y = train[\"pv_measurement\"].dropna().values\n",
    "    time_diff = train[\"time\"].diff().mean().total_seconds()\n",
    "    sampling_rate = 1 / time_diff\n",
    "\n",
    "    n = len(y)\n",
    "    freq = np.fft.fftfreq(n, 1 / sampling_rate)\n",
    "    fft_y = np.fft.fft(y)\n",
    "    amp_fft_y = np.abs(fft_y)\n",
    "    phase = np.angle(fft_y)\n",
    "    return freq, fft_y, amp_fft_y, phase, sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = { 'A': train_a, 'B': train_b, 'C': train_c }\n",
    "locations = trains.keys()\n",
    "\n",
    "freqs, fft_ys, amp_fft_ys, phases, sampling_rates = [\n",
    "    { \n",
    "        loc: get_fft_transforms(trains[loc])[k] for loc in locations \n",
    "    } \n",
    "    for k in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], amp_fft_ys[loc][:len(freqs[loc])//2])\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], 20 * np.log10(amp_fft_ys[loc][:len(freqs[loc])//2]))\n",
    "    plt.title(f\"Spectrum in magnitude of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude (dB)\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looked on the most important frequencies in those spectrums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_peak_frequencies(amp_fft_y, freq, threshold, loc):\n",
    "    peaks, _ = scipy.signal.find_peaks(amp_fft_y[:len(amp_fft_y)//2], height=threshold)\n",
    "    peak_frequencies = freq[:len(freq)//2][peaks]\n",
    "\n",
    "    period_size = int(1/peak_frequencies[0])\n",
    "    continuous_component = np.mean(trains[loc][\"pv_measurement\"].dropna().values[:period_size])\n",
    "\n",
    "    print(\"Location:\", loc)\n",
    "    print(f'Most important periods (in days): \\n{1 / peak_frequencies / 3600 / 24}')\n",
    "    print(f'Value of the continous component: {continuous_component}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = { 'A': .5e7, 'B': .5e6, 'C': .25e6 }\n",
    "for loc in locations:\n",
    "    print_peak_frequencies(amp_fft_ys[loc], freqs[loc], thresholds[loc], loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First commentaries:\n",
    "\n",
    "We know from the analysis of the nan values that A got the most clean datas in term of `pv_measurement` values. So our analysis will mostly be based on what we see on A. We can notice 3 most important frequencies: one for the year, one for the day and one for a half-day (12 hours). If we look more on the frequency plot, we can notice a most little one frequency (that our threshold impeach us to read it on the last print). This seems to be a peak for a period of 8 hours, according to the code cell bellow.\n",
    "\n",
    "Because B and C are not much clean, we can suppose that the big differencies we found with A comes from the Nan values, which create some empty cells in these frames, which are compensated by increasing the frequency values. However, we did not pay attention to it much at first be because most of our analysis were based on A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_a_1 = np.min(np.where(freqs['A'] > .00003)) \n",
    "freq_a_2 = np.max(np.where(freqs['A'] < .00004))\n",
    "freq_arg = np.argmax(fft_ys['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2])\n",
    "1 / freqs['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2][freq_arg] / 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm what we sayied on B and C compared to A if we look on the differents sampling rates depending on the situation. Theorically, it should be close to one hour ($=3600$ seconds) because our values are measured every hours. But if we look on `1 / sampling_rates['B']` and `1 / sampling_rates['C']` we see that it's more than it for B and C locations. This comes from Nan values and confirms our point above.\n",
    "\n",
    "We can notice that `1 / sampling_rates['B']` is a bit bigger than an hour. We can explain it by the gap of one week between `X_train_observed_a` and `X_train_estimated_a`, which exists as well in `train_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / sampling_rates['A'] / 3600, 1 / sampling_rates['B'] / 3600, 1 / sampling_rates['C'] / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can try is to recalculate the model by the inverse of Fourier's transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fft_ys = { loc: np.fft.ifft(fft_ys[loc]) for loc in locations }\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(i_fft_ys[loc])\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsolar",
   "language": "python",
   "name": "mlsolar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
