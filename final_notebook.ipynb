{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning\n",
    "## Final Notebook of project\n",
    "\n",
    "\n",
    "Students: TESTARD Arthur, VERDON Valentin and VERDIER Nahel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "# \n",
    "0. [Imports and data initialization](#0-imports-and-data-initialization)\n",
    "1. [Some Data analysis](#1-data-analysis)\n",
    "    <!-- 1. [Sub paragraph](#2-research-leads) -->\n",
    "2. [Research leads](#2-research-leads)\n",
    "    1. [Signal analysis](#21-signal-analysis)\n",
    "3. [Preprocessing](#3-preprocessing)\n",
    "    1. [Management of Nan values](#31-management-of-nan-values)\n",
    "    2. [Normalization](#32-normalization)\n",
    "    3. [Feature engineering](#33-feature-engineering)\n",
    "    4. [Management of constant parts of pv_measurement on B and C](#34-management-of-constant-parts-of-pv_measurement-on-b-and-c)\n",
    "    5. [Interpolation of the output values](#35-interpolation-of-the-output-values)\n",
    "    6. [Input reshaping](#36-input-reshaping)\n",
    "    7. [Pre-processing function](#37-preprocessing-function)\n",
    "4. [Pre-processing ideas that didn't work](#4-pre-processing-ideas-that-didnt-work)\n",
    "    1. [Polynomial features](#41-polynomial-features)\n",
    "    2. [Mean of features](#42-mean-of-features)\n",
    "5. [Separation of training and test sets](#5-separation-of-training-and-test-sets)\n",
    "    1. [Split training and test sets on estimated set dates](#51-split-training-and-test-sets-on-estimated-set-dates)\n",
    "    2. [Split training and test sets on prediction dates range and estimated set](#52-split-training-and-test-sets-on-prediction-dates-range-and-estimated-set)\n",
    "6. [Models](#6-models)\n",
    "    1. [Models performance comparison](#61-models-performance-comparison)\n",
    "    1. [Hyper-parameters selction](#62-hyper-parameters)\n",
    "<!-- 7. [Another paragraph](#part7) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, learning_curve, train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFolder:\n",
    "    def __init__(self, folder_name: str):\n",
    "        self.folder_name: str\n",
    "        self.X_test_estimated: str = f\"{folder_name}/X_test_estimated.parquet\"\n",
    "        self.X_train_estimated: str = f\"{folder_name}/X_train_estimated.parquet\"\n",
    "        self.X_train_observed: str = f\"{folder_name}/X_train_observed.parquet\"\n",
    "        self.train_targets: str | None = f\"{folder_name}/train_targets.parquet\"\n",
    "\n",
    "A = DataFolder(folder_name='A')\n",
    "B = DataFolder(folder_name='B')\n",
    "C = DataFolder(folder_name='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(diff_path: str = ''):\n",
    "    train_a = pd.read_parquet(diff_path + A.train_targets)\n",
    "    train_b = pd.read_parquet(diff_path + B.train_targets)\n",
    "    train_c = pd.read_parquet(diff_path + C.train_targets)\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet(diff_path + A.X_train_estimated)\n",
    "    X_train_estimated_b = pd.read_parquet(diff_path + B.X_train_estimated)\n",
    "    X_train_estimated_c = pd.read_parquet(diff_path + C.X_train_estimated)\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet(diff_path + A.X_train_observed)\n",
    "    X_train_observed_b = pd.read_parquet(diff_path + B.X_train_observed)\n",
    "    X_train_observed_c = pd.read_parquet(diff_path + C.X_train_observed)\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet(diff_path + A.X_test_estimated)\n",
    "    X_test_estimated_b = pd.read_parquet(diff_path + B.X_test_estimated)\n",
    "    X_test_estimated_c = pd.read_parquet(diff_path + C.X_test_estimated)\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research leads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Signal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data were presenting some periodicities, intuitively, one of our first idea were to analyse the different signals we have, starting by our target, `pv_measurement`. However, as we can see on the following plot, the data is not completly cleared, specially on B and C. We will come back to this point in Preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dates_when_constants(df, date_c = 'time', y = 'pv_measurement', delta = { 'days': 3 }):\n",
    "    df = df.copy()\n",
    "    mask_y_change = df[y] != df[y].shift(1)\n",
    "\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    constant_periods = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if not mask_y_change[index]:\n",
    "            if start_date is None:\n",
    "                start_date = row[date_c]\n",
    "            end_date = row[date_c]\n",
    "        else:\n",
    "            if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "                constant_periods.append((start_date, end_date))\n",
    "            start_date = None\n",
    "            end_date = None\n",
    "\n",
    "    if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "        constant_periods.append((start_date, end_date))\n",
    "    return constant_periods\n",
    "\n",
    "def delete_date_range_from_df(df, dates, date_c = 'time'):\n",
    "    df = df.copy()\n",
    "    c = 0\n",
    "    for start_date, end_date in dates:\n",
    "        mask = (df[date_c] >= start_date) & (df[date_c] < end_date)\n",
    "        df = df[~mask]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "delta = { 'hours': 12 * 4}\n",
    "train_a = delete_date_range_from_df(train_a, filter_dates_when_constants(train_a, delta=delta))\n",
    "train_b = delete_date_range_from_df(train_b, filter_dates_when_constants(train_b, delta=delta))\n",
    "train_c = delete_date_range_from_df(train_c, filter_dates_when_constants(train_c, delta=delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(30, 20), sharex=True)\n",
    "\n",
    "train_a[['time','pv_measurement']].set_index('time').plot(ax=axs[0], title='pv_measurement on location A')\n",
    "train_b[['time','pv_measurement']].set_index('time').plot(ax=axs[1], title='pv_measurement on location B')\n",
    "train_c[['time','pv_measurement']].set_index('time').plot(ax=axs[2], title='pv_measurement on location C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, we then tryied to analys this signal with basic components such as Fourier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_transforms(train):\n",
    "    y = train[\"pv_measurement\"].dropna().values\n",
    "    time_diff = train[\"time\"].diff().mean().total_seconds()\n",
    "    sampling_rate = 1 / time_diff\n",
    "\n",
    "    n = len(y)\n",
    "    freq = np.fft.fftfreq(n, 1 / sampling_rate)\n",
    "    fft_y = np.fft.fft(y)\n",
    "    amp_fft_y = np.abs(fft_y)\n",
    "    phase = np.angle(fft_y)\n",
    "    return freq, fft_y, amp_fft_y, phase, sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = { 'A': train_a.dropna(subset='pv_measurement'), 'B': train_b.dropna(subset='pv_measurement'), 'C': train_c.dropna(subset='pv_measurement') }\n",
    "locations = trains.keys()\n",
    "\n",
    "freqs, fft_ys, amp_fft_ys, phases, sampling_rates = [\n",
    "    { \n",
    "        loc: get_fft_transforms(trains[loc])[k] for loc in locations \n",
    "    } \n",
    "    for k in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], amp_fft_ys[loc][:len(freqs[loc])//2])\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(freqs[loc][:len(freqs[loc])//2], 20 * np.log10(amp_fft_ys[loc][:len(freqs[loc])//2]))\n",
    "    plt.title(f\"Spectrum in magnitude of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude (dB)\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looked on the most important frequencies in those spectrums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_peak_frequencies(amp_fft_y, freq, threshold, loc):\n",
    "    peaks, _ = scipy.signal.find_peaks(amp_fft_y[:len(amp_fft_y)//2], height=threshold)\n",
    "    peak_frequencies = freq[:len(freq)//2][peaks]\n",
    "\n",
    "    period_size = int(1/peak_frequencies[0])\n",
    "    continuous_component = np.mean(trains[loc][\"pv_measurement\"].dropna().values[:period_size])\n",
    "\n",
    "    print(\"Location:\", loc)\n",
    "    print(f'Most important periods (in days): \\n{1 / peak_frequencies / 3600 / 24}')\n",
    "    print(f'Value of the continous component: {continuous_component}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = { 'A': .5e7, 'B': .5e6, 'C': .25e6 }\n",
    "for loc in locations:\n",
    "    print_peak_frequencies(amp_fft_ys[loc], freqs[loc], thresholds[loc], loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First commentaries:\n",
    "\n",
    "We know from the analysis of the nan values that A got the most clean datas in term of `pv_measurement` values. So our analysis will mostly be based on what we see on A. We can notice 3 most important frequencies: one for the year, one for the day and one for a half-day (12 hours). If we look more on the frequency plot, we can notice a most little one frequency (that our threshold impeach us to read it on the last print). This seems to be a peak for a period of 8 hours, according to the code cell bellow.\n",
    "\n",
    "Because B and C are not much clean, we can suppose that the big differencies we found with A comes from the Nan values, which create some empty cells in these frames, which are compensated by increasing the frequency values. However, we did not pay attention to it much at first be because most of our analysis were based on A data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_a_1 = np.min(np.where(freqs['A'] > .00003)) \n",
    "freq_a_2 = np.max(np.where(freqs['A'] < .00004))\n",
    "freq_arg = np.argmax(fft_ys['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2])\n",
    "1 / freqs['A'][:len(fft_ys['A'])//2][freq_a_1:freq_a_2][freq_arg] / 3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm what we sayied on B and C compared to A if we look on the differents sampling rates depending on the situation. Theorically, it should be close to one hour ($=3600$ seconds) because our values are measured every hours. But if we look on `1 / sampling_rates['B']` and `1 / sampling_rates['C']` we see that it's more than it for B and C locations. This comes from Nan values and confirms our point above.\n",
    "\n",
    "We can notice that `1 / sampling_rates['B']` is a bit bigger than an hour. We can explain it by the gap of one week between `X_train_observed_a` and `X_train_estimated_a`, which exists as well in `train_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / sampling_rates['A'] / 3600, 1 / sampling_rates['B'] / 3600, 1 / sampling_rates['C'] / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can try is to recalculate the model by the inverse of Fourier's transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_fft_ys = { loc: np.fft.ifft(fft_ys[loc]) for loc in locations }\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "k = 1\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, k)\n",
    "    plt.plot(trains[loc]['pv_measurement'], label='real pv_measurement')\n",
    "    plt.plot(i_fft_ys[loc], label='ifft pv_measurement')\n",
    "    plt.title(f\"Spectrum of the pv_measurement at {loc} location\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a gap created in C but in facts its due to the index. \n",
    "\n",
    "Now the idea is to keep only the most important frequencies in order to have a model which can be written like this:\n",
    "$$y[n] = \\hat{y}[n] + r[n]$$\n",
    "\n",
    "where $n$ is the index of the output, $y[n]$ is the real value of `pv_measurement` at index $n$ (or time $t$), $\\hat{y}[n]$ is the value at index $n$ of the signal filtered predicted by signal analysis and $r[n]$ is the value at index $n$ of the noise created by mostly, the weather, from our inputs `X_train_estimated`, `X_train_observed`, etc. It would be design by a machine learning model. Actually we did not had the time to test this feature entirely, because of a lack of time our goals priotization. So, it is not entirely designed, but we will detail as far as we came to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = { 'A': '2022-10-21', 'B': '2022-03-15', 'C': '2022-04-01'}\n",
    "# end_dates = { 'A': X_train_observed_a['date_forecast'].max(), 'B': X_train_observed_b['date_forecast'].max(), 'C': X_train_observed_c['date_forecast'].max() }\n",
    "\n",
    "start_dates = { 'A': '2020-10-21', 'B': '2020-03-15', 'C': '2020-04-01' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(fft_values, threshold=60, sample_rate=1):\n",
    "\n",
    "    n = len(fft_values)\n",
    "\n",
    "    frequencies = np.fft.fftfreq(n, 1 / sample_rate)\n",
    "    amplitudes = fft_values * (np.abs(fft_values) > threshold)\n",
    "    phases = np.angle(fft_values)\n",
    "    return {\"frequencies\": frequencies, \"amplitudes\": amplitudes, \"phases\": phases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_signal(model, duration,sample_rate):\n",
    "    frequencies = model[\"frequencies\"]\n",
    "    amplitudes = model[\"amplitudes\"]\n",
    "    phases = model[\"phases\"]\n",
    "\n",
    "    t = np.arange(0, duration, 1)\n",
    "    signal = np.zeros(len(t), dtype=np.complex128)\n",
    "    \n",
    "    for freq, amp, phase in tqdm(zip(frequencies, amplitudes, phases)):\n",
    "        signal += amp * np.exp(2j * np.pi * freq * t / sample_rate + phase)\n",
    "    return signal / len(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholds_to_get_n_freq(signal, nb_freq, threshold, step):\n",
    "    assert step > 0\n",
    "    fft = np.fft.fft(signal)\n",
    "    abs_fft = np.abs(fft[:len(fft)//2])\n",
    "\n",
    "    freqs = [ f for f in abs_fft if f > threshold ]\n",
    "    threshold += step\n",
    "    while len(freqs) > nb_freq:\n",
    "        freqs = [ f for f in abs_fft if f > threshold ]\n",
    "        threshold += step\n",
    "    threshold = threshold if len(freqs) > 0 else threshold - step\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtred_signal(signal, nb_freqs, sample_rate, nb_days_to_predict = 0, threshold = 0, scaler = StandardScaler):\n",
    "    scaler_pred = scaler\n",
    "    scaler = scaler()\n",
    "    Y_normed = scaler.fit_transform(np.array(signal['pv_measurement'].dropna()).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    threshold = get_thresholds_to_get_n_freq(signal=Y_normed, nb_freq=nb_freqs, threshold=0, step=.5)\n",
    "    model = get_model(fft_values=np.fft.fft(Y_normed), threshold=threshold, sample_rate=sample_rate)\n",
    "    pred_from_model_data = np.real(reconstruct_signal(model, duration=len(model[\"frequencies\"]) + nb_days_to_predict, sample_rate=sample_rate)) \n",
    "    scaler_pred = scaler_pred()\n",
    "    pred_normed = scaler_pred.fit_transform(pred_from_model_data.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    Y_filtred = scaler.inverse_transform(pred_normed.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    # If we want to filter negative values\n",
    "    Y_filtred[Y_filtred < 0] = 0\n",
    "    return Y_filtred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_freqs = 2\n",
    "nb_days_to_predict = 0\n",
    "\n",
    "trains_on_dates = { loc: trains[loc][(trains[loc][\"time\"] < pd.Timestamp(end_dates[loc])) & (trains[loc][\"time\"] >= pd.Timestamp(start_dates[loc]))] for loc in locations }\n",
    "Y_filtred = { loc: get_filtred_signal(trains_on_dates[loc], nb_freqs, sampling_rates[loc], nb_days_to_predict=nb_days_to_predict) for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(Y_filtred[loc], color='b')\n",
    "    plt.plot(np.array(trains_on_dates[loc]['pv_measurement']), color='orange')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we explored different ways to design $\\hat{y}[n]$. The first one is a raw filter on the whole signal. This method were not much efficient. In our researchs we found `prophet`, a Python (and R) library which gives a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = { 'A': X_train_estimated_a['date_forecast'].max(), 'B': X_train_estimated_b['date_forecast'].max(), 'C': X_train_estimated_c['date_forecast'].max() }\n",
    "trains = { loc: trains[loc][trains[loc]['time'] <= end_dates[loc]] for loc in locations}\n",
    "\n",
    "prophet_scalers = { loc: MinMaxScaler() for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_raw_prophet = { loc: trains[loc].dropna(subset='pv_measurement').reset_index().rename(columns={'time': 'ds', 'pv_measurement': 'y'}) for loc in locations }\n",
    "\n",
    "trains_prophet = trains_raw_prophet\n",
    "for loc in locations:\n",
    "    trains_prophet[loc]['y'] = prophet_scalers[loc].fit_transform(np.array(trains_raw_prophet[loc]['y']).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "models_prophet = { loc: Prophet(changepoint_prior_scale=0.05) for loc in locations }\n",
    "predictions_prophet = {}\n",
    "forecast = {}\n",
    "\n",
    "for loc in locations:\n",
    "    models_prophet[loc].fit(trains_prophet[loc])\n",
    "    predictions_prophet[loc] = models_prophet[loc].make_future_dataframe(periods=66, freq='h')\n",
    "    forecast[loc] = models_prophet[loc].predict(predictions_prophet[loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(50, 30))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(trains_prophet[loc]['y'], color='orange')\n",
    "    plt.plot(forecast[loc]['yhat'], color='b')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results, from prophet and the filter signal, are not really satisfying. Then came the idea, inpired by [this paper](https://peerj.com/preprints/3190.pdf), to see what's happen if we plot one signal for each hour (it would make $24 * 3 = 72$ models). We then first split our signals by hours and plot what we get with prophet prediction and our filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFULL VALUES TO COMPILE\n",
    "hours = [ f\"0{h}\" if h < 10 else str(h) for h in range(24) ]\n",
    "\n",
    "nb_freqs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_on_dates = { loc: trains[loc][(trains[loc][\"time\"] < pd.Timestamp(end_dates[loc])) & (trains[loc][\"time\"] >= pd.Timestamp(start_dates[loc]))] for loc in locations }\n",
    "trains_on_hours = { loc: { h: trains_on_dates[loc][trains_on_dates[loc]['time'].dt.strftime('%H:%M:%S').str.endswith(f'{h}:00:00')] for h in hours } for loc in locations }\n",
    "\n",
    "# MAYBE DROP NA ON B AND C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_h_filtred = { loc: { h: get_filtred_signal(trains_on_hours[loc][h], nb_freqs, sampling_rates[loc] * 24, nb_days_to_predict=nb_days_to_predict, scaler=StandardScaler) for h in hours} for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(trains_on_hours[loc][h]['pv_measurement']), color='orange')\n",
    "        plt.plot(Y_h_filtred[loc][h], color='b')\n",
    "        plt.title(f\"Filtred signal at time h={h} for location {loc}\")\n",
    "        sp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hours_to_days(signal):\n",
    "    min_len = np.min([len(signal[h]) for h in hours ])\n",
    "    y_pred = []\n",
    "    for d in range(min_len):\n",
    "        for h in hours:\n",
    "            y_pred.append(signal[h][d])\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_train = { loc: convert_hours_to_days(Y_h_filtred[loc]) for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(reconstructed_train[loc], color='b', label='reconstruted from signal analysis')\n",
    "    plt.plot(np.array(trains_on_dates[loc]['pv_measurement']), color='orange', label='real value')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    plt.legend()\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get here a far more satisfying result. There is a problem for B, we did not get why the curve does not go to 0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = { loc: {} for loc in locations }\n",
    "trains_prophet_on_hours = { loc: { h: trains_prophet[loc][trains_prophet[loc]['ds'].dt.strftime('%H:%M:%S').str.endswith(f'{h}:00:00')] for h in hours } for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        date_index = [ (pd.Timestamp(start_dates[loc]) + d).strftime(\"%Y-%m-%d\") for d in [ timedelta(days=k) for k in range(len(trains_prophet_on_hours[loc][h])) ] ]\n",
    "        trains_prophet_on_hours[loc][h] = pd.DataFrame({'ds': date_index, 'y': trains_prophet_on_hours[loc][h]['y']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_prophet = { loc: { h: Prophet(changepoint_prior_scale=0.05) for h in hours} for loc in locations }\n",
    "predictions_prophet = { loc: {} for loc in locations }\n",
    "forecast = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        models_prophet[loc][h].fit(trains_prophet_on_hours[loc][h])\n",
    "        predictions_prophet[loc][h] = models_prophet[loc][h].make_future_dataframe(periods=0, freq='h')\n",
    "        forecast[loc][h] = models_prophet[loc][h].predict(predictions_prophet[loc][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(trains_prophet_on_hours[loc][h].reset_index()['y']), color='orange')\n",
    "        plt.plot(np.array(forecast[loc][h]['yhat']), color='b')\n",
    "        plt.title(f\"Prophet on location {loc}\")\n",
    "        sp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_to_reconstruct = { loc: { h: prophet_scalers[loc].inverse_transform(np.array(forecast[loc][h]['yhat']).reshape(-1, 1)).reshape(-1) for h in hours } for loc in locations }\n",
    "Y_to_reconstruct = { loc: { h: np.array(forecast[loc][h]['yhat']) for h in hours } for loc in locations }\n",
    "reconstructed_train_prophet = { loc: convert_hours_to_days(Y_to_reconstruct[loc]) for loc in locations }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = 1\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "for loc in locations:\n",
    "    plt.subplot(3, 1, sp)\n",
    "    plt.plot(reconstructed_train_prophet[loc], color='b', label='reconstruted from signal analysis')\n",
    "    plt.plot(np.array(trains_prophet[loc]['y']), color='orange', label='real value')\n",
    "    plt.title(f\"Prophet on location {loc}\")\n",
    "    plt.legend()\n",
    "    sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are less satisfying than the precedent result. But let's see what happens if we try to predict the noise, $r[n]$, with Prophet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prophet = { loc: { h: np.array(trains_on_hours[loc][h]['pv_measurement']) - Y_h_filtred[loc][h] for h in hours } for loc in locations }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prophet = { loc: { h: np.array(trains_on_hours[loc][h]['pv_measurement']) - Y_h_filtred[loc][h] for h in hours } for loc in locations }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_noise_prophet_on_hours = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        date_index = [ (pd.Timestamp(start_dates[loc]) + d).strftime(\"%Y-%m-%d\") for d in [ timedelta(days=k) for k in range(len(noise_prophet[loc][h])) ] ]\n",
    "        trains_noise_prophet_on_hours[loc][h] = pd.DataFrame({'ds': date_index, 'y': noise_prophet[loc][h]})\n",
    "\n",
    "models_prophet = { loc: { h: Prophet(changepoint_prior_scale=0.05) for h in hours} for loc in locations }\n",
    "predictions_prophet = { loc: {} for loc in locations }\n",
    "forecast = { loc: {} for loc in locations }\n",
    "\n",
    "for loc in locations:\n",
    "    for h in hours:\n",
    "        models_prophet[loc][h].fit(trains_noise_prophet_on_hours[loc][h])\n",
    "        predictions_prophet[loc][h] = models_prophet[loc][h].make_future_dataframe(periods=0, freq='h')\n",
    "        forecast[loc][h] = models_prophet[loc][h].predict(predictions_prophet[loc][h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in locations:\n",
    "    sp = 1\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    for h in hours:\n",
    "        plt.subplot(4, 6, sp)\n",
    "        plt.plot(np.array(noise_prophet[loc][h]), color='orange')\n",
    "        plt.plot(np.array(forecast[loc][h]['yhat']), color='b')\n",
    "        plt.title(f\"Prophet on location {loc}\")\n",
    "        sp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our study, we explored many ways to pre-process our inputs. Some were compatible to each other, some were not. We are going to present in this part, all we did as pre-process and those we used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Management of NAN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, we have 3 variables with Nan values: `snow_density:kgm3`,`ceiling_height_agl:m`,  `cloud_base_agl:m`.\n",
    "\n",
    "\n",
    "Since `snow_density:kgm3` is more than 95% empty, we're going to delete it because it's unusable.\n",
    "\n",
    "\n",
    "As for the other two, we've seen that they have missing values, probably due to measurement errors. We'll simply perform a **linear interpolation**, taking the average of the before and after values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gestion_nan(df):\n",
    "  df_copy = df.copy()\n",
    "  #delete of the snow density column\n",
    "  df_copy = df_copy.drop('snow_density:kgm3',axis=1)\n",
    "  # Approximation of the other two columns\n",
    "  df_copy['ceiling_height_agl:m'] = df_copy['ceiling_height_agl:m'].interpolate(method='linear', limit_direction='both')\n",
    "  df_copy['cloud_base_agl:m'] = df_copy['cloud_base_agl:m'].interpolate(method='linear', limit_direction='both')\n",
    "  return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data have variable ranges that are quite different from one another. So we're going to do some normalization to solve this problem.\n",
    "\n",
    "To do this, we're going to use different normalizations, and when we train the model we'll see which normalization corresponds best to our dataset.\n",
    "\n",
    "The various standardizations used are as follows: StandardScaler, Normalizer, MinMaxScaler from the library sklearn.preprocessing\n",
    "\n",
    "Here's the typical function we'll use to normalize the data.\n",
    "\n",
    "Note: it adapts according to the train and test set, since the scaler must be the same for both stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_z_score_normalize_dataframe(df,return_scaler=False,scaler=None,fit=True):\n",
    "    \"\"\"\n",
    "    Normalizes a DataFrame using z-score normalization (mean and standard deviation) from Scikit-Learn.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The z-score normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if scaler == None :\n",
    "      # Create a StandardScaler instance\n",
    "      scaler = StandardScaler()\n",
    "\n",
    "      # Fit the scaler on the DataFrame and transform the data\n",
    "      normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "    else :\n",
    "      if fit:\n",
    "        normalized_data = scaler.fit_transform(df)\n",
    "      else:\n",
    "        normalized_data = scaler.transform(df)\n",
    "\n",
    "    # Create a new DataFrame with the scaled data\n",
    "    normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "    # retourner le scaler\n",
    "    if return_scaler :\n",
    "      return normalized_df,scaler\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is made up of many features.\n",
    "\n",
    "**Delating Features**:\n",
    "\n",
    "Some of them are less important than others. We'll take a look at the ranking of feature importance given by our models after training.\n",
    "\n",
    "For the moment, however, we've chosen to delete the `date_calc` feature, which doesn't seem relevant because we only see it in the *estimated* dataset.\n",
    "\n",
    "**Creating Features**:\n",
    "\n",
    "As for the `date_forecast` feature. We saw during the signal processing analysis that this feature contains events that repeat at given periods (years/days), so we're going to split our feature into several sub-features to try and obtain relationships with the target.\n",
    "\n",
    "We therefore separate according to: *hours, day of the week, season, month, year, day of the year and day of the month*.\n",
    "\n",
    "**Modification of Features**:\n",
    "\n",
    "When we explore our data we can see that `sun_azimuth:d` corresponds to the position of the sun in degrees. So we can see that 0 and 360 correspond to the same thing. So we're going to transform this feature using a cosine to make it continuous and consistent with what it represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction = X_train_observed_a['sun_azimuth:d'][0:199]\n",
    "transfo = extraction.apply(lambda x : np.cos(x/180*np.pi + np.pi))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(X_train_observed_a['date_forecast'][0:199],extraction/180-1,label=\"old\",ls=\"--\")\n",
    "plt.plot(X_train_observed_a['date_forecast'][0:199],transfo,label=\"new\",ls=\"--\")\n",
    "plt.plot(train_a['time'][0:49],train_a[\"pv_measurement\"][0:49]/2500-1,label=\"target\",alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"sun_azimuth modification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, label):\n",
    "\n",
    "    # creating features from dateforecast\n",
    "    df['hour'] = df[\"date_forecast\"].dt.hour\n",
    "    df['dayofweek'] = df[\"date_forecast\"].dt.dayofweek\n",
    "    df['quarter'] = df[\"date_forecast\"].dt.quarter\n",
    "    df['month'] = df[\"date_forecast\"].dt.month\n",
    "    df['year'] = df[\"date_forecast\"].dt.year\n",
    "    df['dayofyear'] = df[\"date_forecast\"].dt.dayofyear\n",
    "    df['dayofmonth'] = df[\"date_forecast\"].dt.day\n",
    "\n",
    "    # Deleting date_calc and date_forecast\n",
    "    df = df.drop([\"date_calc\",\"date_forecast\"],axis=1) if \"date_calc\" in df.columns else df.drop([\"date_forecast\"],axis=1) # date de calcul des estimated inutil pour l'instant\n",
    "\n",
    "    # Modification of sun_azimuth\n",
    "    df['sun_azimuth:d'] = df['sun_azimuth:d'].apply(lambda x : np.cos(x/180*np.pi + np.pi))\n",
    "\n",
    "    # return the target if necessary\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        df = df.drop(label,axis=1)\n",
    "        return df, y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Management of constant parts of pv_measurement on B and C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look closely at the shape of pv_measurement on B and C, we can see that there are certain sections during which the feature doesn't vary even at night when it should.\n",
    "\n",
    "We therefore assume that these are erroneous measurements that need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dates_when_constants(df, date_c = 'time', y = 'pv_measurement', delta = { 'days': 3 }):\n",
    "    df = df.copy()\n",
    "    mask_y_change = df[y] != df[y].shift(1)\n",
    "\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    constant_periods = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if not mask_y_change[index]:\n",
    "            if start_date is None:\n",
    "                start_date = row[date_c]\n",
    "            end_date = row[date_c]\n",
    "        else:\n",
    "            if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "                constant_periods.append((start_date, end_date))\n",
    "            start_date = None\n",
    "            end_date = None\n",
    "\n",
    "    if start_date is not None and (end_date - start_date) >= pd.Timedelta(**delta):\n",
    "        constant_periods.append((start_date, end_date))\n",
    "    return constant_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_date_range_from_df(df, dates, date_c = 'time'):\n",
    "    df = df.copy()\n",
    "    c = 0\n",
    "    for start_date, end_date in dates:\n",
    "        mask = (df[date_c] >= start_date) & (df[date_c] < end_date)\n",
    "        df = df[~mask]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum duration\n",
    "delta = { 'hours': 12 * 4}\n",
    "\n",
    "# list of these\n",
    "filter_dates_when_constants(train_b, delta=delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion B\n",
    "train_b_delete = delete_date_range_from_df(train_b, filter_dates_when_constants(train_b, delta=delta))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(train_b_delete[\"time\"],train_b_delete[\"pv_measurement\"],s=0.2)\n",
    "plt.grid()\n",
    "plt.title(\"B without constant periods\")\n",
    "plt.show()\n",
    "\n",
    "# Deletion C\n",
    "train_c_delete = delete_date_range_from_df(train_c, filter_dates_when_constants(train_c, delta=delta))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(train_c_delete[\"time\"],train_c_delete[\"pv_measurement\"],s=0.2)\n",
    "plt.grid()\n",
    "plt.title(\"C without constant periods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Interpolation of the output values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our idea, which worked pretty well, was to interpolate the values of the output. We tryed different interpolations, but the one which looked as the best one to do so, were the linear one. It's mathically defined as following, and the function which do the job, is `interpolate_output_values`.$$y(t + 1/4) = 0.75 * y(t) + 0.25 * y(t+1)$$\n",
    "$$y(t + 1/2) = 0.5 * y(t) + 0.5 * y(t+1)$$\n",
    "$$y(t + 3/4) = 0.25 * y(t) + 0.75 * y(t+1)$$\n",
    "where t is a round hour (such as 12:00:00, 13:00:00 etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_method = 'linear'\n",
    "def interpolate_output_values(df):\n",
    "    freq = '15T'\n",
    "    df = df.set_index('time').resample(freq).asfreq()\n",
    "    df['pv_measurement'] = df['pv_measurement'].interpolate(method=interpolation_method)\n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_interpolated = interpolate_output_values(train_a)\n",
    "train_b_interpolated = interpolate_output_values(train_b)\n",
    "train_c_interpolated = interpolate_output_values(train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_range = train_a['time'].loc[3000:3023].index\n",
    "interpolation_range = train_a['time'].loc[4 * 3000: 4 * 3023].index\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(train_a['time'][basic_range], train_a['pv_measurement'][basic_range], label='pv_measurement values on A')\n",
    "plt.scatter(train_a_interpolated['time'][interpolation_range], train_a_interpolated['pv_measurement'][interpolation_range], color='orange', label='quarter-hourly linear-interpolation')\n",
    "plt.scatter(train_a['time'][basic_range], train_a['pv_measurement'][basic_range], label='pv_measurement values on A')\n",
    "plt.title('pv_measurement on A values and linear interpolation values of its self')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpolation permit us to create 4 times more data, on every location, which are, in a restrective way, similar to the original curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Input reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also thought about some ways to see from a different point of view the data. One was to create more inputs from the data of the past hour. Firstly, we had to select which one of the 4 lines is the most important one to predict the output. We suggested that the solar panel were quite reactive, which means that, for every hours, we have to take into consideration the value of the current hour. It means, for example, at anyday, for an output at the 12:00:00, we have to consider the values from X at 12:00:00. And, because we want to consider what happened during the last hour, we had, as new columns, the values at 11:15:00, 11:30:00 and 11:45:00. The code to create a such matrix is defined as `reshape_frame_to_match_output` (our advise is to not run this function because it require a lot of time to reshape the inputs, we recognize that its not much optimized). \n",
    "\n",
    "We had to take some others assumptions into consideration as well. For example, we have in the estimated-testing sets, the time line is not continuous. For example, some days are skipped, so we designed it such as, it duplicate the value of 23:45:00 every days (in order to not get nan values), and the input at 00:00:00 are taken 4 times every days. We considered that, that point was not much important to duplicate the data, because the sun's data are nill (there is no sun during the night as far we experimented it yet), so it should not influence much the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def reshape_frame_to_match_output(frame_input):\n",
    "    frame = frame_input[1:]\n",
    "    groups = [frame[i:i+4] for i in range(0, len(frame), 4)]\n",
    "\n",
    "    aggregated_groups = []\n",
    "    \n",
    "    first_input = pd.DataFrame(frame_input.loc[0])\n",
    "\n",
    "    group = pd.concat([first_input, first_input, first_input, first_input])\n",
    "    group_without_nan = group.fillna('')\n",
    "    new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "    aggregated_groups.append(new_input)     \n",
    "    \n",
    "    for group in tqdm(groups):\n",
    "        if (len(np.array(group['date_forecast'])) > 3):\n",
    "            if ((np.array(group['date_forecast']))[3].astype(datetime).time() == datetime(year=1970, month=1, day=1, hour=0, minute=0, second=0).time()):\n",
    "                group.reset_index(drop=True, inplace=True)\n",
    "                temp_group = group.loc[3]\n",
    "                group.loc[3] = group.loc[2]\n",
    "\n",
    "                group_without_nan = group.fillna('')\n",
    "                new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "                aggregated_groups.append(new_input)\n",
    "\n",
    "                group.loc[0] = temp_group\n",
    "                group.loc[1] = temp_group\n",
    "                group.loc[2] = temp_group\n",
    "                group.loc[3] = temp_group\n",
    "\n",
    "                group_without_nan = group.fillna('')\n",
    "                new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "                aggregated_groups.append(new_input)\n",
    "            else:\n",
    "                group_without_nan = group.fillna('')\n",
    "                new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "                # if group == groups[0]: print(len(new_input))\n",
    "                aggregated_groups.append(new_input)\n",
    "        else:\n",
    "            group.reset_index(drop=True, inplace=True)\n",
    "            for k in range(4 - len(group)):\n",
    "                group = pd.concat([group, pd.DataFrame(group.loc[len(group) - 1]).T], ignore_index=True)\n",
    "            group_without_nan = group.fillna('')\n",
    "            new_input = group_without_nan.stack().reset_index(drop=True)\n",
    "            aggregated_groups.append(new_input)\n",
    "            \n",
    "    new_table = pd.concat(aggregated_groups, axis=1, ignore_index=False).T\n",
    "\n",
    "    columns = []\n",
    "    for k in range(4):\n",
    "        [columns.append(f\"{c}_{k}\") for c in frame.keys()]\n",
    "\n",
    "    new_table.columns = columns\n",
    "    new_table.reset_index(drop=True, inplace=True)\n",
    "    return new_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is called and run the following way (in comment because it takes to much time to run). We also added a function to save this dataframes to not have to run it everytimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_estimated_a_reshaped = reshape_frame_to_match_output(X_train_estimated_a)\n",
    "# X_train_observed_a_reshaped = reshape_frame_to_match_output(X_train_observed_a)\n",
    "# X_test_estimated_a_reshaped = reshape_frame_to_match_output(X_test_estimated_a)\n",
    "\n",
    "# X_train_estimated_b_reshaped = reshape_frame_to_match_output(X_train_estimated_b)\n",
    "# X_train_observed_b_reshaped = reshape_frame_to_match_output(X_train_observed_b)\n",
    "# X_test_estimated_b_reshaped = reshape_frame_to_match_output(X_test_estimated_b)\n",
    "\n",
    "# X_train_estimated_c_reshaped = reshape_frame_to_match_output(X_train_estimated_c)\n",
    "# X_train_observed_c_reshaped = reshape_frame_to_match_output(X_train_observed_c)\n",
    "# X_test_estimated_c_reshaped = reshape_frame_to_match_output(X_test_estimated_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_names = ['X_train_estimated.parquet', 'X_train_observed.parquet', 'X_test_estimated.parquet']\n",
    "# folders = [ f\"{loc}_reshaped/\" for loc in locations ]\n",
    "# X_reshaped = [ X_train_estimated_a_reshaped, X_train_observed_a_reshaped, X_test_estimated_a_reshaped, X_train_estimated_b_reshaped, X_train_observed_b_reshaped, X_test_estimated_b_reshaped, X_train_estimated_c_reshaped, X_train_observed_c_reshaped, X_test_estimated_c_reshaped]\n",
    "\n",
    "# for folder in folders:\n",
    "#     for file in files_names:\n",
    "#         index_in_X = (files_names.index(file) + 1) * (folders.index(folder) + 1) - 1\n",
    "#         X_reshaped[index_in_X].replace('', np.nan).to_parquet(folder + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_preprocess(A = [X_train_observed_a, X_train_estimated_a], B = [X_train_observed_b, X_train_estimated_b], C = [X_train_observed_c, X_train_estimated_c]):\n",
    "    X_total_a = pd.concat(A, ignore_index=True)\n",
    "    X_total_b = pd.concat(B, ignore_index=True)\n",
    "    X_total_c = pd.concat(C, ignore_index=True)\n",
    "    X_total_a_y = pd.merge(X_total_a, train_a.rename(columns={'time': 'date_forecast'}, inplace=False).dropna(subset='pv_measurement', inplace=False), on='date_forecast', how='inner')\n",
    "    X_total_b_y = pd.merge(X_total_b, train_a.rename(columns={'time': 'date_forecast'}, inplace=False).dropna(subset='pv_measurement', inplace=False), on='date_forecast', how='inner')\n",
    "    X_total_c_y = pd.merge(X_total_c, train_a.rename(columns={'time': 'date_forecast'}, inplace=False).dropna(subset='pv_measurement', inplace=False), on='date_forecast', how='inner')\n",
    "    X_total_a_y_nan = gestion_nan(X_total_a_y)\n",
    "    X_total_b_y_nan = gestion_nan(X_total_b_y)\n",
    "    X_total_c_y_nan = gestion_nan(X_total_c_y)\n",
    "\n",
    "    return X_total_a_y_nan, X_total_b_y_nan, X_total_c_y_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total_a_y_nan, X_total_b_y_nan, X_total_c_y_nan = partial_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-processing ideas that didn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find new relationships between features, we're going to apply a method called `Polynomial Features`, which consists of adding new features to the dataset that are the products of those already present.\n",
    "\n",
    "In our dataset, we currently have over 50 features. We can't take all the combinations, as too many features would prevent the model from finding good combinations (with very long training times). So we're going to run a first model without `Polynomial Features` to get a ranking of the importance of the features, so as to take only a certain number. This is what the `recup_var` function does.\n",
    "\n",
    "We then add to our dataframe the polynomial combinations of these features to the desired degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recup_var(reg,number=10):\n",
    "  feature_importance = reg.get_booster().get_fscore()\n",
    "  sorted_feature_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_features = [feature for feature, _ in sorted_feature_importance[:number]]\n",
    "  return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_filter(df,columns):\n",
    "  df_filtre = df[columns]\n",
    "  return df_filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_df_polynomial_feature(df,degre=1):\n",
    "\n",
    "  # Creation of a PolynomialFeatures object\n",
    "  poly = PolynomialFeatures(degre, include_bias=False)\n",
    "\n",
    "  # Apply the polynomial caractristiques to the datas\n",
    "  poly_features = poly.fit_transform(df)\n",
    "\n",
    "  # Create a new dataframe\n",
    "  poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(input_features=df.columns))\n",
    "\n",
    "  return poly_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the results obtained by testing several combinations of features did not improve the score. We therefore decided to shelve this avenue.\n",
    "\n",
    "If you'd like to implement it and test it with our code, all you have to do is train a model and put it into the recup_var function, then retrieve the filtered dataframe with the filter_column function. Finally, just put it into the creation_df_polynomial_feature function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mean of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that dataframes containing features are sliced by quarter-hour, whereas those containing targets are sliced by hour.\n",
    "\n",
    "To try and take into account what happens in the hour as weather (as it impacts `pv_measurement`), we tried to take the average of the different weather components in the hour.\n",
    "\n",
    "Please note that in the test file, the days are taken at random and we start our predictions at 0h, so we need to take this into account. This is why our code will follow the following algorithm every 24h:\n",
    "* take the weather at 0h\n",
    "* remove the weather at 0h and 23h15/30/45\n",
    "* add the average weather for the 4 lines every 4 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traitement_df(df_input):\n",
    "\n",
    "    if \"date_calc\" in list(df_input.columns):\n",
    "      df_input = df_input.drop([\"date_calc\"],axis=1)\n",
    "\n",
    "    df_output = pd.DataFrame(columns=df_input.columns)\n",
    "\n",
    "    aux = df_input[df_input.index % 4 == 0]\n",
    "    date_col = aux[\"date_forecast\"]\n",
    "    date_col.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_input = df_input.drop([\"date_forecast\"],axis=1)\n",
    "\n",
    "    for i in range(0, len(df_input), 96):\n",
    "        df_etude = df_input.iloc[i:i+96]\n",
    "\n",
    "        first_row = df_etude.iloc[0]\n",
    "        df_output = pd.concat([df_output, first_row.to_frame().transpose()], ignore_index=True)\n",
    "\n",
    "        df_etude = df_etude.iloc[1:-3]\n",
    "        df_etude.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        means = df_etude.groupby(df_etude.index // 4).mean()\n",
    "        df_output = pd.concat([df_output, means], ignore_index=True)\n",
    "\n",
    "    df_output[\"date_forecast\"] = date_col\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Separation of training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our study, we used different ways to split the training and test sets. We found out that some ways were more efficient than some others. We first used a \"classical approach\", which consisted by taking last 10% of the whole training set (observed and estimated) for each locations. We then realized that, because our predictions would be during a date range of spring/summer, we must had to include some dates from this time in our test set. Moreover, our predictions will be based on estimated values, which means that we have to take that kind of values into consideration, in order to get the distribution of estimated values in our prevision as well.\n",
    "\n",
    "So we are going to present the first split we did, which were less efficient, than the second one that we are going to present then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Split training and test sets on estimated set dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_testing_sets_on_test_set_dates():\n",
    "    quantile = .25\n",
    "    pv_train_a = X_total_a_y_nan[X_total_a_y_nan['date_forecast'] <= X_train_estimated_a['date_forecast'].quantile(quantile)].copy()\n",
    "    pv_test_a = X_total_a_y_nan[X_total_a_y_nan['date_forecast'] > X_train_estimated_a['date_forecast'].quantile(quantile)].copy()\n",
    "\n",
    "    pv_train_b = X_total_b_y_nan[X_total_b_y_nan['date_forecast'] <= X_train_estimated_b['date_forecast'].quantile(quantile)].copy()\n",
    "    pv_test_b = X_total_b_y_nan[X_total_b_y_nan['date_forecast'] > X_train_estimated_b['date_forecast'].quantile(quantile)].copy()\n",
    "\n",
    "    pv_train_c = X_total_c_y_nan[X_total_c_y_nan['date_forecast'] <= X_train_estimated_c['date_forecast'].quantile(quantile)].copy()\n",
    "    pv_test_c = X_total_c_y_nan[X_total_c_y_nan['date_forecast'] > X_train_estimated_c['date_forecast'].quantile(quantile)].copy() \n",
    "    return pv_train_a, pv_test_a, pv_train_b, pv_test_b, pv_train_c, pv_test_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Split training and test sets on prediction dates range and estimated set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_2020 = pd.to_datetime(\"2020-04-21\")\n",
    "end_2020 = pd.to_datetime(\"2020-07-22\")\n",
    "\n",
    "start_2021 = pd.to_datetime(\"2021-04-21\")\n",
    "end_2021 = pd.to_datetime(\"2021-07-22\")\n",
    "\n",
    "start_2022 = pd.to_datetime(\"2022-04-21\")\n",
    "end_2022 = pd.to_datetime(\"2022-07-22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_for_split_training_and_testing(df, start_estimated, time_column = 'date_forecast'):\n",
    "    mask_2020 = ((df[time_column] >= start_2020) & (df[time_column] < end_2020))\n",
    "    mask_2021 = ((df[time_column] >= start_2021) & (df[time_column] < end_2021))\n",
    "    mask_2022 = ((df[time_column] >= start_2022) & (df[time_column] < end_2022))\n",
    "    mask_estimated = (df[time_column] >= start_estimated)\n",
    "    return mask_2020, mask_2021, mask_2022,mask_estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_testing_set(df, start_estimated, random_state=42, test_size = .1, time_column = 'date_forecast'):\n",
    "    df_to_split = df.copy()\n",
    "    mask_2020, mask_2021, mask_2022, mask_estimated = create_mask_for_split_training_and_testing(df_to_split, start_estimated)\n",
    "    df_summers = df_to_split[ mask_2020 | mask_2021 | mask_2022 | mask_estimated]\n",
    "    df_not_summer = df_to_split[~(mask_2020 | mask_2021 | mask_2022 | mask_estimated)]\n",
    "    test_size = test_size * (len(df_summers) + len(df_not_summer)) / len(df_summers)\n",
    "    train_data_summer, pv_test_not_ordered = train_test_split(df_summers, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    pv_train_not_ordered = pd.concat([train_data_summer, df_not_summer])\n",
    "    pv_train = pv_train_not_ordered.sort_values(by='date_forecast')\n",
    "    pv_test = pv_test_not_ordered.sort_values(by='date_forecast')\n",
    "    return pv_train, pv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "pv_train_a, pv_test_a = split_training_testing_set(X_total_a_y_nan, X_train_estimated_a[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_b, pv_test_b = split_training_testing_set(X_total_b_y_nan, X_train_estimated_b[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_c, pv_test_c = split_training_testing_set(X_total_c_y_nan, X_train_estimated_c[\"date_forecast\"].mean(), random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_a :\",pv_train_a.shape)\n",
    "print(\"test_a :\",pv_test_a.shape)\n",
    "print(\"Ratio test/total :\", round(pv_test_a.shape[0]/(pv_test_a.shape[0]+pv_train_a.shape[0]),3)*100, '%')\n",
    "print(\"train_b :\",pv_train_b.shape)\n",
    "print(\"test_b :\",pv_test_b.shape)\n",
    "print(\"Ratio test/total :\", round(pv_test_b.shape[0]/(pv_test_b.shape[0]+pv_train_b.shape[0]),3)*100, '%')\n",
    "print(\"train_c :\",pv_train_c.shape)\n",
    "print(\"test_c :\",pv_test_c.shape)\n",
    "print(\"Ratio test/total :\", round(pv_test_c.shape[0]/(pv_test_c.shape[0]+pv_train_c.shape[0]),3)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Models performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the reading of our study, we will only detail our results for location A.\n",
    "\n",
    "What's more, A has the highest target values (4 to 5 times higher than B and C), which gives us a good idea of the trend to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datas\n",
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()\n",
    "\n",
    "# observed + estimated\n",
    "X_total_a = pd.concat([X_train_observed_a,X_train_estimated_a])\n",
    "X_total_b = pd.concat([X_train_observed_b,X_train_estimated_b])\n",
    "X_total_c = pd.concat([X_train_observed_c,X_train_estimated_c])\n",
    "\n",
    "# Rename + Delete lignes without pv_measurement\n",
    "train_a.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_b.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_c.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "train_a.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_b.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_c.dropna(subset=['pv_measurement'], inplace=True)\n",
    "\n",
    "# Match between features and target\n",
    "X_total_a_y = pd.merge(X_total_a, train_a, on='date_forecast', how='inner')\n",
    "X_total_b_y = pd.merge(X_total_b, train_b, on='date_forecast', how='inner')\n",
    "X_total_c_y = pd.merge(X_total_c, train_c, on='date_forecast', how='inner')\n",
    "\n",
    "# Nan\n",
    "X_total_a_y_nan = gestion_nan(X_total_a_y)\n",
    "X_total_b_y_nan = gestion_nan(X_total_b_y)\n",
    "X_total_c_y_nan = gestion_nan(X_total_c_y)\n",
    "\n",
    "# Train/Test split\n",
    "split_date_a = X_train_estimated_a['date_forecast'].quantile(0.25)\n",
    "split_date_b = X_train_estimated_b['date_forecast'].quantile(0.25)\n",
    "split_date_c = X_train_estimated_c['date_forecast'].quantile(0.25)\n",
    "\n",
    "pv_train_a, pv_test_a, pv_train_b, pv_test_b, pv_train_c, pv_test_c = split_training_testing_sets_on_test_set_dates()\n",
    "\n",
    "# Date_forecast type\n",
    "pv_train_a['date_forecast'] = pd.to_datetime(pv_train_a['date_forecast'])\n",
    "pv_test_a['date_forecast'] = pd.to_datetime(pv_test_a['date_forecast'])\n",
    "\n",
    "pv_train_b['date_forecast'] = pd.to_datetime(pv_train_b['date_forecast'])\n",
    "pv_test_b['date_forecast'] = pd.to_datetime(pv_test_b['date_forecast'])\n",
    "\n",
    "pv_train_c['date_forecast'] = pd.to_datetime(pv_train_c['date_forecast'])\n",
    "pv_test_c['date_forecast'] = pd.to_datetime(pv_test_c['date_forecast'])\n",
    "\n",
    "\n",
    "#feature Engineering\n",
    "X_train_a, y_train_a = create_features(pv_train_a, label='pv_measurement')\n",
    "X_test_a, y_test_a = create_features(pv_test_a, label='pv_measurement')\n",
    "\n",
    "X_train_b, y_train_b = create_features(pv_train_b, label='pv_measurement')\n",
    "X_test_b, y_test_b = create_features(pv_test_b, label='pv_measurement')\n",
    "\n",
    "X_train_c, y_train_c = create_features(pv_train_c, label='pv_measurement')\n",
    "X_test_c, y_test_c = create_features(pv_test_c, label='pv_measurement')\n",
    "\n",
    "# Normalisation\n",
    "scaler_a = StandardScaler()\n",
    "scaler_b = StandardScaler()\n",
    "scaler_c = StandardScaler()\n",
    "\n",
    "X_train_a_norm,scaler_a = sklearn_z_score_normalize_dataframe(X_train_a,return_scaler=True, scaler=scaler_a)\n",
    "X_train_b_norm,scaler_b = sklearn_z_score_normalize_dataframe(X_train_b,return_scaler=True, scaler=scaler_b)\n",
    "X_train_c_norm,scaler_c = sklearn_z_score_normalize_dataframe(X_train_c,return_scaler=True, scaler=scaler_c)\n",
    "\n",
    "X_test_a_norm = sklearn_z_score_normalize_dataframe(X_test_a,return_scaler=False,scaler=scaler_a, fit=False)\n",
    "X_test_b_norm = sklearn_z_score_normalize_dataframe(X_test_b,return_scaler=False,scaler=scaler_b, fit=False)\n",
    "X_test_c_norm = sklearn_z_score_normalize_dataframe(X_test_c,return_scaler=False,scaler=scaler_c, fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42) #, max_depth = 10\n",
    "rf_model.fit(X_train_a_norm, y_train_a)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42) #learning_rate=0.1,\n",
    "gb_model.fit(X_train_a_norm, y_train_a)\n",
    "\n",
    "# Crez un modle XGBoost\n",
    "reg_a = xgb.XGBRegressor(n_estimators=100)\n",
    "reg_a.fit(X_train_a_norm, y_train_a,\n",
    "          eval_set=[(X_train_a_norm, y_train_a), (X_test_a_norm, y_test_a)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT BOOSTING\n",
    "# Liste pour stocker les erreurs de test  chaque itration\n",
    "valid_errors = []\n",
    "\n",
    "for i, y_pred in enumerate(gb_model.staged_predict(X_test_a_norm)):\n",
    "    valid_errors.append(mean_squared_error(y_test_a, y_pred))\n",
    "\n",
    "# Crez une figure pour chaque taux d'apprentissage\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Learning curve Gradient Boosting\")\n",
    "plt.plot(\n",
    "    np.arange(100) + 1,\n",
    "    gb_model.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set Deviance\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(100) + 1, valid_errors, \"r-\", label=\"Test Set Deviance\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# XGBOOST\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "results = reg_a.evals_result()\n",
    "train_errors = results['validation_0'][\"rmse\"]\n",
    "test_errors = results['validation_1'][\"rmse\"]\n",
    "\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(\"rmse\")\n",
    "plt.legend()\n",
    "plt.title('Learning Curves XGBoost')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "rf_predictions = rf_model.predict(X_test_a_norm)\n",
    "gb_predictions = gb_model.predict(X_test_a_norm)\n",
    "reg_a_predictions = reg_a.predict(X_test_a_norm)\n",
    "\n",
    "# Evaluation\n",
    "rf_mse = mean_squared_error(y_test_a, rf_predictions)\n",
    "gb_mse = mean_squared_error(y_test_a, gb_predictions)\n",
    "reg_a_mse = mean_squared_error(y_test_a, reg_a_predictions)\n",
    "print(f\"Random Forest MSE: {round(rf_mse,0)}\")\n",
    "print(f\"Gradient Boosting MSE: {round(gb_mse,0)}\")\n",
    "print(f\"XGBoost MSE: {round(reg_a_mse,0)}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "mae_rf = np.mean(np.abs(y_test_a - rf_predictions))\n",
    "mae_gb = np.mean(np.abs(y_test_a - gb_predictions))\n",
    "mae_reg_a = np.mean(np.abs(y_test_a - reg_a_predictions))\n",
    "print(f\"Random Forest MAE: {round(mae_rf,2)}\")\n",
    "print(f\"Gradient Boosting MAE: {round(mae_gb,2)}\")\n",
    "print(f\"XGBoost MAE: {round(mae_reg_a,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these first results, we can see that the models have converged well (see learning curves above). We first chose to train our models using RMSE, as it allows us to take better account of outliers (if we look at the distribution of pv_measurement we see that values can reach large values, but quite rarely).\n",
    "\n",
    "In the evaluation phase, we can see that on both measures, the XGBoost model performs much better than the other two models.\n",
    "\n",
    "In addition to this study, we can look at the relative importance of each variable on the model by plotting the importance graph generated by the \"tree\" type models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus decided to focus on a XGBoost model, which implIES creating models (for A, B and C) with the best hyper-parameters. We thus made the following code in order to find the best hyper-parameters. The thing is, that algorithm has an exponential complexity in terms of the number of hyper-parameters we want to try. \n",
    "Thus to make it converge, we were tooking the last best-values, and thus we tryed the neighboors values. And we reapeated it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datas\n",
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = read_files()\n",
    "\n",
    "# Interpolate values\n",
    "train_a = interpolate_output_values(train_a)\n",
    "train_b = interpolate_output_values(train_b)\n",
    "train_c = interpolate_output_values(train_c)\n",
    "\n",
    "# observed + estimated\n",
    "X_total_a = pd.concat([X_train_observed_a,X_train_estimated_a])\n",
    "X_total_b = pd.concat([X_train_observed_b,X_train_estimated_b])\n",
    "X_total_c = pd.concat([X_train_observed_c,X_train_estimated_c])\n",
    "\n",
    "# Rename + Delete lignes without pv_measurement\n",
    "train_a.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_b.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "train_c.rename(columns={'time': 'date_forecast'}, inplace=True)\n",
    "\n",
    "train_a.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_b.dropna(subset=['pv_measurement'], inplace=True)\n",
    "train_c.dropna(subset=['pv_measurement'], inplace=True)\n",
    "# Match between features and target\n",
    "X_total_a_y = pd.merge(X_total_a, train_a, on='date_forecast', how='inner')\n",
    "X_total_b_y = pd.merge(X_total_b, train_b, on='date_forecast', how='inner')\n",
    "X_total_c_y = pd.merge(X_total_c, train_c, on='date_forecast', how='inner')\n",
    "\n",
    "# Nan\n",
    "X_total_a_y_nan = gestion_nan(X_total_a_y)\n",
    "X_total_b_y_nan = gestion_nan(X_total_b_y)\n",
    "X_total_c_y_nan = gestion_nan(X_total_c_y)\n",
    "\n",
    "# Train/Test split\n",
    "pv_train_a, pv_test_a = split_training_testing_set(X_total_a_y_nan, X_train_estimated_a[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_b, pv_test_b = split_training_testing_set(X_total_b_y_nan, X_train_estimated_b[\"date_forecast\"].mean(), random_state=random_state)\n",
    "pv_train_c, pv_test_c = split_training_testing_set(X_total_c_y_nan, X_train_estimated_c[\"date_forecast\"].mean(), random_state=random_state)\n",
    "\n",
    "# Date_forecast type\n",
    "pv_train_a['date_forecast'] = pd.to_datetime(pv_train_a['date_forecast'])\n",
    "pv_test_a['date_forecast'] = pd.to_datetime(pv_test_a['date_forecast'])\n",
    "\n",
    "pv_train_b['date_forecast'] = pd.to_datetime(pv_train_b['date_forecast'])\n",
    "pv_test_b['date_forecast'] = pd.to_datetime(pv_test_b['date_forecast'])\n",
    "\n",
    "pv_train_c['date_forecast'] = pd.to_datetime(pv_train_c['date_forecast'])\n",
    "pv_test_c['date_forecast'] = pd.to_datetime(pv_test_c['date_forecast'])\n",
    "\n",
    "\n",
    "#feature Engineering\n",
    "X_train_a, y_train_a = create_features(pv_train_a, label='pv_measurement')\n",
    "X_test_a, y_test_a = create_features(pv_test_a, label='pv_measurement')\n",
    "\n",
    "X_train_b, y_train_b = create_features(pv_train_b, label='pv_measurement')\n",
    "X_test_b, y_test_b = create_features(pv_test_b, label='pv_measurement')\n",
    "\n",
    "X_train_c, y_train_c = create_features(pv_train_c, label='pv_measurement')\n",
    "X_test_c, y_test_c = create_features(pv_test_c, label='pv_measurement')\n",
    "\n",
    "# Normalisation\n",
    "scaler_a = StandardScaler()\n",
    "scaler_b = StandardScaler()\n",
    "scaler_c = StandardScaler()\n",
    "\n",
    "X_train_a_norm,scaler_a = sklearn_z_score_normalize_dataframe(X_train_a,return_scaler=True, scaler=scaler_a)\n",
    "X_train_b_norm,scaler_b = sklearn_z_score_normalize_dataframe(X_train_b,return_scaler=True, scaler=scaler_b)\n",
    "X_train_c_norm,scaler_c = sklearn_z_score_normalize_dataframe(X_train_c,return_scaler=True, scaler=scaler_c)\n",
    "\n",
    "X_test_a_norm = sklearn_z_score_normalize_dataframe(X_test_a,return_scaler=False,scaler=scaler_a, fit=False)\n",
    "X_test_b_norm = sklearn_z_score_normalize_dataframe(X_test_b,return_scaler=False,scaler=scaler_b, fit=False)\n",
    "X_test_c_norm = sklearn_z_score_normalize_dataframe(X_test_c,return_scaler=False,scaler=scaler_c, fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE BEST HYPER-PARAMETERS WE FOUND OUT\n",
    "eval_metric = \"rmse\"\n",
    "hyper_params_a = {'subsample': 0.6, 'refresh_leaf': 1, 'n_estimators': 4000, 'min_child_weight': 15, 'max_depth': 4, 'gamma': 0.5, 'eta': 0.005, 'colsample_bytree': 0.8}\n",
    "hyper_params_b = {'n_estimators': 4000, 'subsample': 0.6, 'refresh_leaf': 1, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 0.5, 'eta': 0.01, 'colsample_bytree': 0.6}\n",
    "hyper_params_c = {'n_estimators': 4000, 'subsample': 0.6, 'refresh_leaf': 1, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 0.5, 'eta': 0.05, 'colsample_bytree': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_a = xgb.XGBRegressor(\n",
    "    eval_metric = eval_metric,\n",
    "    **hyper_params_a\n",
    "    )\n",
    "\n",
    "\n",
    "reg_b = xgb.XGBRegressor(\n",
    "    eval_metric = eval_metric,\n",
    "    **hyper_params_b\n",
    "    )\n",
    "\n",
    "reg_c = xgb.XGBRegressor(\n",
    "    eval_metric = eval_metric,\n",
    "    **hyper_params_c\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of hyper-parameters range to find the best suit of it\n",
    "\n",
    "gen_params = {\n",
    "        'n_estimators': [2000, 3000, 3500],\n",
    "        'gamma': [0.5, 1, 5],\n",
    "        'subsample': [0.6, 1.0],\n",
    "        'max_depth': [4, 5],\n",
    "        'eta': [ 0.005, 0.01, 0.05],\n",
    "        'n_estimators': [ 3000, 4000 ]\n",
    "        }\n",
    "\n",
    "params_a = {\n",
    "        'min_child_weight': [ 10, 12, 15 ],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'refresh_leaf': [ .7, 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_b = {\n",
    "        'min_child_weight': [3, 5, 7],\n",
    "        'colsample_bytree': [.7, 0.6, .5],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_c = {\n",
    "        'min_child_weight': [3, 5, 7],\n",
    "        'colsample_bytree': [ 0.6 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of hyper-parameters range, based on the best one founds, \n",
    "# to run more quickly the program\n",
    "\n",
    "gen_params = {\n",
    "        'n_estimators': [5000],\n",
    "        'gamma': [0.5],\n",
    "        'subsample': [0.6],\n",
    "        'n_estimators': [5000]\n",
    "        }\n",
    "\n",
    "params_a = {\n",
    "        'min_child_weight': [ 15, 17 ],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'max_depth': [4],\n",
    "        'eta': [ 0.005 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_b = {\n",
    "        'min_child_weight': [5],\n",
    "        'colsample_bytree': [0.6],\n",
    "        'max_depth': [5],\n",
    "        'eta': [ 0.01 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }\n",
    "\n",
    "params_c = {\n",
    "        'min_child_weight': [5],\n",
    "        'colsample_bytree': [1],\n",
    "        'max_depth': [5],\n",
    "        'eta': [ 0.05 ],\n",
    "        'refresh_leaf': [ 1 ],\n",
    "        **gen_params\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_comb_a = reduce(lambda x,y: x*y, [len(v) for v in params_a.values()])\n",
    "param_comb_b = reduce(lambda x,y: x*y, [len(v) for v in params_b.values()])\n",
    "param_comb_c = reduce(lambda x,y: x*y, [len(v) for v in params_c.values()])\n",
    "n_jobs = 10\n",
    "\n",
    "\n",
    "random_search_a = RandomizedSearchCV(reg_a, param_distributions=params_a, n_iter=param_comb_a, scoring='neg_mean_squared_error', n_jobs=n_jobs, cv=2, verbose=3, random_state=1001 )\n",
    "random_search_b = RandomizedSearchCV(reg_b, param_distributions=params_b, n_iter=param_comb_b, scoring='neg_mean_squared_error', n_jobs=n_jobs, cv=2, verbose=3, random_state=1001 )\n",
    "random_search_c = RandomizedSearchCV(reg_c, param_distributions=params_c, n_iter=param_comb_c, scoring='neg_mean_squared_error', n_jobs=n_jobs, cv=2, verbose=3, random_state=1001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_a.fit(\n",
    "    X_train_a_norm, y_train_a,\n",
    "    eval_set=[(X_train_a_norm, y_train_a), (X_test_a_norm, y_test_a)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n All results:')\n",
    "print('\\n Best estimator:')\n",
    "print(random_search_a.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search_a.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_b.fit(\n",
    "    X_train_b_norm, y_train_b,\n",
    "    eval_set=[(X_train_b_norm, y_train_b), (X_test_b_norm, y_test_b)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n All results:')\n",
    "print(random_search_b.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search_b.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search_b.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_c.fit(\n",
    "    X_train_c_norm, y_train_c,\n",
    "    eval_set=[(X_train_c_norm, y_train_c), (X_test_c_norm, y_test_c)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n All results:')\n",
    "print(random_search_c.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search_c.best_estimator_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search_c.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_a = random_search_a.best_estimator_\n",
    "reg_b = random_search_b.best_estimator_\n",
    "reg_c = random_search_c.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_a.fit(X_train_a_norm, y_train_a,\n",
    "          eval_set=[(X_train_a_norm, y_train_a), (X_test_a_norm, y_test_a)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )\n",
    "reg_b.fit(X_train_b_norm, y_train_b,\n",
    "          eval_set=[(X_train_b_norm, y_train_b), (X_test_b_norm, y_test_b)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )\n",
    "reg_c.fit(X_train_c_norm, y_train_c,\n",
    "          eval_set=[(X_train_c_norm, y_train_c), (X_test_c_norm, y_test_c)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=True,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crez des listes vides pour stocker les erreurs d'entranement et de test\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Accdez aux erreurs d'entranement et de test aprs chaque itration\n",
    "results = reg_a.evals_result()\n",
    "train_errors = results['validation_0'][eval_metric]\n",
    "test_errors = results['validation_1'][eval_metric]\n",
    "\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(eval_metric)\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()\n",
    "\n",
    "min_error_a = min(test_errors)\n",
    "# Crez des listes vides pour stocker les erreurs d'entranement et de test\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Accdez aux erreurs d'entranement et de test aprs chaque itration\n",
    "results = reg_b.evals_result()\n",
    "train_errors = results['validation_0'][eval_metric]\n",
    "test_errors = results['validation_1'][eval_metric]\n",
    "\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(eval_metric)\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()\n",
    "\n",
    "min_error_b = min(test_errors)\n",
    "# Crez des listes vides pour stocker les erreurs d'entranement et de test\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Accdez aux erreurs d'entranement et de test aprs chaque itration\n",
    "results = reg_c.evals_result()\n",
    "train_errors = results['validation_0'][eval_metric]\n",
    "test_errors = results['validation_1'][eval_metric]\n",
    "\n",
    "min_error_c = min(test_errors)\n",
    "# Tracez les courbes d'apprentissage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train')\n",
    "plt.plot(test_errors, label='Test')\n",
    "plt.xlabel('Boosting Rounds')\n",
    "plt.ylabel(eval_metric)\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. BLABLABLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsolar",
   "language": "python",
   "name": "mlsolar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
